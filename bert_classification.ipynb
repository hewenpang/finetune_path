{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hewenpang/finetune_path/blob/main/bert_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-12-18T01:52:26.335793Z",
          "start_time": "2024-12-18T01:52:26.333297Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_id",
        "outputId": "de4e33da-c79c-4893-d9f4-4680fc012cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import  AutoTokenizer,AutoModelForSequenceClassification\n",
        "!pip install datasets\n",
        "from datasets import  load_dataset\n",
        "import pandas as pd\n",
        "from torch.utils.data import  DataLoader,Dataset"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "class My_dataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.data = pd.read_csv('./ChnSentiCorp_htl_all.csv')\n",
        "        self.data = self.data.dropna()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data.iloc[index]['review'],self.data.iloc[index]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:00:17.889474Z",
          "start_time": "2024-12-18T02:00:17.883390Z"
        },
        "id": "647048e197ba1174"
      },
      "id": "647048e197ba1174",
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较为简单.', 1)\n",
            "('商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!', 1)\n",
            "('早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。', 1)\n",
            "('宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小，但加上低价位因素，还是无超所值的；环境不错，就在小胡同内，安静整洁，暖气好足-_-||。。。呵还有一大优势就是从宾馆出发，步行不到十分钟就可以到梅兰芳故居等等，京味小胡同，北海距离好近呢。总之，不错。推荐给节约消费的自助游朋友~比较划算，附近特色小吃很多~', 1)\n",
            "('CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风', 1)\n"
          ]
        }
      ],
      "source": [
        "dataset = My_dataset()\n",
        "for i in range(5):\n",
        "    print(dataset[i])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:00:58.404955Z",
          "start_time": "2024-12-18T02:00:58.354541Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84423374bcedcde",
        "outputId": "12b560fd-3c4f-48c8-b092-fdceec43daed"
      },
      "id": "84423374bcedcde",
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7ca2c8940f70>\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_data,test_data = random_split(dataset,lengths=[0.8,0.2])\n",
        "\n",
        "print(train_data)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:06:28.143220Z",
          "start_time": "2024-12-18T02:06:28.137067Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35728c012c4ae7da",
        "outputId": "4b16b061-edc9-47c3-efe7-c0b67ff28545"
      },
      "id": "35728c012c4ae7da",
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model  = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-uncased',num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts,labels = [],[]\n",
        "    for item in batch:\n",
        "        texts.append(item[0])\n",
        "        labels.append(item[1])\n",
        "    inputs = tokenizer(texts,padding='max_length',max_length=128,truncation=True,return_tensors='pt')\n",
        "    inputs['labels'] = torch.tensor(labels)\n",
        "    return inputs\n",
        "\n",
        "train_loader = DataLoader(train_data,batch_size=32,shuffle=True,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data,batch_size=64,shuffle=True,collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:33:01.293611Z",
          "start_time": "2024-12-18T02:33:00.375588Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bddbc807b1cc4d45",
        "outputId": "bfc59916-b15a-464b-a6fc-2a79342fa2b4"
      },
      "id": "bddbc807b1cc4d45",
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "model  = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-uncased',num_labels=2)"
      ],
      "metadata": {
        "id": "tzK27LmukQnk"
      },
      "id": "tzK27LmukQnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101,  100,  100,  ..., 1743, 1989,  102],\n",
              "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
              "        [ 101, 1802,  100,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [ 101, 1744,  100,  ...,    0,    0,    0],\n",
              "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
              "        [ 101,  100,  100,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "        1, 0, 1, 1, 1, 1, 1, 1])}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "next(enumerate(train_loader))[1]"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:33:04.038621Z",
          "start_time": "2024-12-18T02:33:04.019431Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e2923a23caa87f9",
        "outputId": "ade9b82d-cd64-441a-cee4-afcf2e780ca0"
      },
      "id": "5e2923a23caa87f9",
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(),lr=2e-5)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:34:14.303102Z",
          "start_time": "2024-12-18T02:34:14.011479Z"
        },
        "id": "76d46f0ffaa2999a"
      },
      "id": "76d46f0ffaa2999a",
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "import  evaluate\n",
        "metric = evaluate.combine(['accuracy','f1'])\n",
        "def eval():\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for batch in test_loader:\n",
        "            if torch.cuda.is_available():\n",
        "              batch = {k:v.cuda() for k,v in batch.items()}\n",
        "            output = model(**batch)\n",
        "            pred = torch.argmax(output.logits,dim=-1)\n",
        "            metric.add_batch(pred,batch['labels'])\n",
        "    model.train()\n",
        "    return metric.compute()\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-18T02:59:14.746514Z",
          "start_time": "2024-12-18T02:55:06.510930Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4feab865d44e72a",
        "outputId": "dd9c13a7-c3ce-4e20-a79d-6cb4342dacd1"
      },
      "id": "c4feab865d44e72a",
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "# 直接用与训练模型进行推理 并且evaluate\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "for batch in test_loader:\n",
        "    if torch.cuda.is_available():\n",
        "        batch = {k:v.cuda() for k,v in batch.items()}\n",
        "    output = model(**batch)\n",
        "    pred = torch.argmax(output.logits,dim=-1)\n",
        "    metric.add_batch(pred,batch['labels'])\n",
        "print(metric.compute())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df_h1QWKXxNt",
        "outputId": "74026686-0f6c-4b17-e499-52b8cdcb2917"
      },
      "id": "Df_h1QWKXxNt",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.6535737282678686, 'f1': 0.7756463719766472}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_params(model):\n",
        "    return {name: param.data.clone() for name, param in model.named_parameters()}\n",
        "\n",
        "def compute_param_diff(params_before, params_after):\n",
        "    diff = {}\n",
        "    for name in params_before:\n",
        "        diff[name] = torch.abs(params_after[name] - params_before[name]).mean().item()\n",
        "    return diff\n",
        "\n",
        "def train(epoch=3,log_steps=100):\n",
        "    global_step = 0\n",
        "    params_before = get_model_params(model)\n",
        "    for ep in range(epoch):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            if torch.cuda.is_available():\n",
        "              batch = {k:v.cuda() for k,v in batch.items()}\n",
        "            optimizer.zero_grad()\n",
        "            output = model(**batch)\n",
        "            output.loss.backward()\n",
        "            optimizer.step()\n",
        "            if global_step % log_steps == 0:\n",
        "                print(f\"ep:{ep},global_step:{global_step},loss:{output.loss.item():.4f}\")\n",
        "            global_step+=1\n",
        "        acc = eval()\n",
        "        print(f\"ep:{ep},acc:{acc}\")\n",
        "    params_after = get_model_params(model)\n",
        "    param_diff = compute_param_diff(params_before, params_after)\n",
        "    print(\"Parameter changes after training:\")\n",
        "    for name, diff in param_diff.items():\n",
        "        print(f\"{name}: {diff:.6f}\")\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCkZCuE-Xn4B",
        "outputId": "a47b33b4-4800-4bae-88dd-2aec455ba7b8"
      },
      "id": "iCkZCuE-Xn4B",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep:0,global_step:0,loss:0.6387\n",
            "ep:0,global_step:100,loss:0.6841\n",
            "ep:0,acc:{'accuracy': 0.6535737282678686, 'f1': 0.7756463719766472}\n",
            "ep:1,global_step:200,loss:0.6992\n",
            "ep:1,global_step:300,loss:0.6762\n",
            "ep:1,acc:{'accuracy': 0.6535737282678686, 'f1': 0.7756463719766472}\n",
            "ep:2,global_step:400,loss:0.6611\n",
            "ep:2,global_step:500,loss:0.6556\n",
            "ep:2,acc:{'accuracy': 0.6535737282678686, 'f1': 0.7756463719766472}\n",
            "Parameter changes after training:\n",
            "bert.embeddings.word_embeddings.weight: 0.000000\n",
            "bert.embeddings.position_embeddings.weight: 0.000000\n",
            "bert.embeddings.token_type_embeddings.weight: 0.000000\n",
            "bert.embeddings.LayerNorm.weight: 0.000000\n",
            "bert.embeddings.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.0.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.0.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.0.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.0.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.0.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.0.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.0.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.0.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.0.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.0.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.0.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.0.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.0.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.0.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.1.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.1.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.1.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.1.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.1.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.1.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.1.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.1.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.1.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.1.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.1.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.1.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.1.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.1.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.2.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.2.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.2.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.2.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.2.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.2.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.2.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.2.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.2.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.2.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.2.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.2.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.2.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.2.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.3.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.3.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.3.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.3.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.3.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.3.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.3.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.3.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.3.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.3.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.3.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.3.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.3.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.3.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.4.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.4.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.4.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.4.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.4.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.4.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.4.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.4.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.4.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.4.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.4.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.4.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.4.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.4.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.5.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.5.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.5.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.5.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.5.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.5.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.5.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.5.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.5.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.5.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.5.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.5.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.5.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.5.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.6.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.6.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.6.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.6.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.6.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.6.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.6.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.6.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.6.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.6.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.6.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.6.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.6.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.6.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.7.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.7.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.7.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.7.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.7.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.7.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.7.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.7.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.7.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.7.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.7.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.7.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.7.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.7.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.8.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.8.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.8.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.8.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.8.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.8.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.8.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.8.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.8.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.8.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.8.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.8.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.8.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.8.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.9.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.9.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.9.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.9.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.9.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.9.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.9.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.9.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.9.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.9.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.9.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.9.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.9.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.9.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.10.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.10.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.10.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.10.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.10.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.10.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.10.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.10.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.10.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.10.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.10.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.10.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.10.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.10.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.11.attention.self.query.weight: 0.000000\n",
            "bert.encoder.layer.11.attention.self.query.bias: 0.000000\n",
            "bert.encoder.layer.11.attention.self.key.weight: 0.000000\n",
            "bert.encoder.layer.11.attention.self.key.bias: 0.000000\n",
            "bert.encoder.layer.11.attention.self.value.weight: 0.000000\n",
            "bert.encoder.layer.11.attention.self.value.bias: 0.000000\n",
            "bert.encoder.layer.11.attention.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.11.attention.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.000000\n",
            "bert.encoder.layer.11.intermediate.dense.weight: 0.000000\n",
            "bert.encoder.layer.11.intermediate.dense.bias: 0.000000\n",
            "bert.encoder.layer.11.output.dense.weight: 0.000000\n",
            "bert.encoder.layer.11.output.dense.bias: 0.000000\n",
            "bert.encoder.layer.11.output.LayerNorm.weight: 0.000000\n",
            "bert.encoder.layer.11.output.LayerNorm.bias: 0.000000\n",
            "bert.pooler.dense.weight: 0.000000\n",
            "bert.pooler.dense.bias: 0.000000\n",
            "classifier.weight: 0.000000\n",
            "classifier.bias: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "5151897426278cd9"
      },
      "id": "5151897426278cd9",
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}